{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08295c33-0193-4e48-9a87-41372be5a604",
   "metadata": {},
   "source": [
    "## Some resources:\n",
    "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a62ac-b343-4df0-ac7a-f0b1852293a9",
   "metadata": {},
   "source": [
    "# What is JAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510779f-0e25-4165-be26-936939de42c4",
   "metadata": {},
   "source": [
    "\"JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\" - JAX docs\n",
    "\n",
    "\n",
    "It has anupdated version of autograd can differentiate native python and numpy code, it can also differentiate through a lot of python features (loops, ifs, recursion, etc.) and keep taking nested derivatives. Supports reverse and forward mode differentiation, can compose both in arbitrary order.\n",
    "\n",
    "JAX uses XLA to compile and run numpy code on accelerators (GPUs, TPUs). happens undere the hood with just-in-time compilation (JIT). Allows for great performance in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcf332c-00fe-40c2-adaf-61de50c0738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d0dab0-9188-4ca0-9373-1150fe0a395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "# Multipyling Matrices\n",
    "# NOTE: Big difference between numpy and JAX on how you generate random numbers\n",
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c786d29a-9a8b-46c6-9df8-1d042e8f5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.6 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU --> not true on mac\n",
    "# Block_until_ready since we are using asynchronous execution by default with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b99a40-333e-4479-ba45-3f8cb188db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 ms ± 3.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Jax numpy functions work on normal numpy arrays but is slower normally since it has to \n",
    "# transfer data to GPU everytime\n",
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79909f5-ad4e-480f-9ef6-2ea5d277e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.6 ms ± 883 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Ensure that NDArray is backed by device memory using device_put()\n",
    "from jax import device_put\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c52fdc-8ac0-43df-9b29-2a6601f80db3",
   "metadata": {},
   "source": [
    "three main program transformations that are useful when writing numerical code:\n",
    "\n",
    "jit(), for speeding up your code\n",
    "    \n",
    "grad(), for taking derivatives\n",
    "    \n",
    "vmap(), for automatic vectorization or batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770ea372-fe43-436a-a9f9-f17f54717ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 ms ± 18.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "641de2d4-12af-4005-8c81-da7a9a5a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 µs ± 5.23 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51e7d0f-f6b9-43fb-8ac5-895b8ef761c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3fae8f8-9a6a-4ba3-811f-5576a2aee736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1964569  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b379a59-8e1c-466b-8bdd-ce42c9b97688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.035325598\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d959be89-1669-4380-910e-86a7e8fe6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes full hessian\n",
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233f360e-4053-467a-b650-5f0783ead3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap: the vectorizing map\n",
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2398989c-d9d3-4d10-8f0a-688353322812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "430 µs ± 546 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76a51675-a807-4617-a410-0c3822140fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "8.03 µs ± 18.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70980388-d63b-465f-ab4c-901c4f1b2e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "12.1 µs ± 15.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
